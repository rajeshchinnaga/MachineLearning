{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7153f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # For general plotting\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import multivariate_normal # MVN not univariate\n",
    "from sklearn.metrics import confusion_matrix #Confusion matrix\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Set seed to generate reproducible \"pseudo-randomness\" (handles scipy's \"randomness\" too)\n",
    "np.random.seed(9)\n",
    "\n",
    "plt.rc('font', size=22)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=18)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=16)    # legend fontsize\n",
    "plt.rc('figure', titlesize=22)   # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1ef34",
   "metadata": {},
   "source": [
    "**Question-1:**\n",
    "\n",
    "Given:\n",
    "1. N = 10000 samples. \n",
    "2. class priors probablities: p(L = 0) = 0.65 and p(L = 1) = 0.35\n",
    "3. Mean vectors:\n",
    "    1. $$ \\mu_{0} = \\begin{bmatrix} -1/2 \\\\ -1/2 \\\\ -1/2 \\end{bmatrix}$$\n",
    "    2. $$ \\mu_{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$$\n",
    "4. Covariance Matrices:\n",
    "    1. $$ \\Sigma_{0} = \\begin{bmatrix} 1 & -0.5 & 0.3 \\\\ -0.5 & 1 & -0.5 \\\\ 0.3 & -0.5 & 1 \\end{bmatrix}$$\n",
    "    1. $$ \\Sigma_{1} = \\begin{bmatrix} 1 & 0.3 & -0.2 \\\\ 0.3 & 1 & 0.3 \\\\ 0.2 & 0.3 & 1 \\end{bmatrix}$$\n",
    "    \n",
    "Generate N independent and identically distributed random 3-D samples from the given Gaussian Probablity Density functions with given class priors probablities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483fb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples to draw from each distribution\n",
    "N = 10000\n",
    "\n",
    "mu = np.array([[-1/2, -1/2, -1/2],\n",
    "               [1, 1, 1]])  # Gaussian distributions means\n",
    "Sigma = np.array([[[1, -0.5, 0.3],\n",
    "                   [-0.5, 1, -0.5],\n",
    "                   [0.3, -0.5, 1]],\n",
    "                  [[1, 0.3, -0.2],\n",
    "                   [0.3, 1, 0.3], \n",
    "                   [-0.2, 0.3, 1]]])  # Gaussian distributions covariance matrices\n",
    "\n",
    "\n",
    "# Determine dimensionality from PDF parameters\n",
    "n = mu.shape[1]\n",
    "\n",
    "# Class priors\n",
    "priors = np.array([0.65, 0.35])  \n",
    "C = len(priors)\n",
    "\n",
    "# Decide randomly which samples will come from each component\n",
    "labels = np.random.rand(N) >= priors[0]\n",
    "L = np.array(range(C))\n",
    "Nl = np.array([sum(labels == i) for i in L])\n",
    "\n",
    "# Draw samples from each class pdf\n",
    "X = np.zeros((N, n))\n",
    "X[labels == 0, :] =  multivariate_normal.rvs(mu[0], Sigma[0], Nl[0])\n",
    "X[labels == 1, :] =  multivariate_normal.rvs(mu[1], Sigma[1], Nl[1])\n",
    "\n",
    "\n",
    "# Plot the original data and their true labels\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "axis = plt.axes(projection='3d')\n",
    "axis.plot(X[labels==0, 0], X[labels==0, 1], X[labels==0, 2],  'ro', label=\"Class 0\")\n",
    "axis.plot(X[labels==1, 0], X[labels==1, 1], X[labels==1, 2], 'go', label=\"Class 1\")\n",
    "\n",
    "plt.legend()\n",
    "axis.set_xlabel(r\"$x_1$\")\n",
    "axis.set_ylabel(r\"$x_2$\")\n",
    "axis.set_zticks([-4, -2, 0, 2, 4])\n",
    "axis.set_zlabel(r\"$x_3$\")\n",
    "plt.title(\"Data and True Class Labels\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Count of Class-0 Samples\", Nl[0])\n",
    "print(\"Count of Class-1 Samples\", Nl[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec727739",
   "metadata": {},
   "source": [
    "**PART- A: Question-1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ba083",
   "metadata": {},
   "source": [
    "Given:\n",
    "$${p(x |L=1) \\over p(x |(L=0)} > \\gamma$$ \n",
    "\n",
    "Solution:\n",
    "ERM CLassifier: with 2 Gaussian Mixture model class conditionals we can derive the above equation to:\n",
    "\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} = \\frac {\\sum_{i=1}^ {\\mu_{1}} \\alpha_{1i} g(x_{i}\\mu_{1i}\\Sigma_{1i})}{\\sum_{i=1}^ {\\mu_{0}} \\alpha_{0i} g(x_{i}\\mu_{0i}\\Sigma_{0i}))}$$\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} = \\frac {g(x|m1,C1)}{g(x|m0,C0)} $$\n",
    "\n",
    "where g(x|m,C) is a multi variate gaussian probablity density function with mean vector m and covariance matrix C.\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} = \\frac {g(x|m1,C1)}{g(x|m0,C0)}  > \\gamma$$\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} = \\frac {g(x|m1,C1)}{g(x|m0,C0)}  > \\gamma = \\frac {p(x|L=0)}{p(x|L=1)} * \\frac {\\delta_{01}-\\delta_{00}}{\\delta_{10}-\\delta_{11}}$$\n",
    "\n",
    "If we consider 1-0 loss matrix then minimum probablity will be:\n",
    "$$ \\gamma = \\frac {p(L=0)}{p(L=1)} $$\n",
    "$$ \\implies \\gamma = \\frac {0.65}{0.35} $$\n",
    "$$ \\implies \\gamma = 1.8572 $$\n",
    "This is the $\\gamma$ value for the lowest likelihood of mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Risk Minimization Classifier (using true model parameters)\n",
    "class_conditional_likelihoods = np.array([multivariate_normal.pdf(X, mu[i], Sigma[i]) for i in L])\n",
    "# Discriminant score is LHS of likelihood-ratio test (LRT) \n",
    "discriminant_score_erm = np.log(class_conditional_likelihoods[1]) - np.log(class_conditional_likelihoods[0])\n",
    "\n",
    "# Gamma threshold for MAP decision rule (same gamma on priors only; 0-1 loss simplification)\n",
    "gamma_map = priors[0] / priors[1]\n",
    "decisions_map = discriminant_score_erm >= np.log(gamma_map)\n",
    "\n",
    "# Get indices and probability estimates of the four decision scenarios:\n",
    "# (true negative, false positive, false negative, true positive)\n",
    "\n",
    "# True Negative Probability\n",
    "ind_00_map = np.argwhere((decisions_map==0) & (labels==0))\n",
    "p_00_map = len(ind_00_map) / Nl[0]\n",
    "# False Positive Probability\n",
    "ind_10_map = np.argwhere((decisions_map==1) & (labels==0))\n",
    "p_10_map = len(ind_10_map) / Nl[0]\n",
    "# False Negative Probability\n",
    "ind_01_map = np.argwhere((decisions_map==0) & (labels==1))\n",
    "p_01_map = len(ind_01_map) / Nl[1]\n",
    "# True Positive Probability\n",
    "ind_11_map = np.argwhere((decisions_map==1) & (labels==1))\n",
    "p_11_map = len(ind_11_map) / Nl[1]\n",
    "\n",
    "# Probability of error for MAP classifier, empirically estimated\n",
    "prob_error_erm = np.array((p_10_map, p_01_map)).dot(Nl.T / N)\n",
    "print(\"Smallest Pr(error) for ERM = {}\".format(prob_error_erm))\n",
    "\n",
    "#Code to flatten the list of lists and storing using numpy array in order to plot in 3D.\n",
    "flat_list =  [item for sublist in ind_00_map for item in sublist]\n",
    "ind_00_map_flat_list = np.array(flat_list)\n",
    "\n",
    "flat_list =  [item for sublist in ind_10_map for item in sublist]\n",
    "ind_10_map_flat_list = np.array(flat_list)\n",
    "\n",
    "flat_list =  [item for sublist in ind_01_map for item in sublist]\n",
    "ind_01_map_flat_list = np.array(flat_list)\n",
    "\n",
    "flat_list =  [item for sublist in ind_11_map for item in sublist]\n",
    "ind_11_map_flat_list = np.array(flat_list)\n",
    "\n",
    "\n",
    "# Plotting in 3D and displaying the MAP Decisions.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "axis = plt.axes(projection='3d')\n",
    "axis.plot(X[ind_00_map_flat_list, 0], X[ind_00_map_flat_list, 1], X[ind_00_map_flat_list, 2],  \n",
    "          'og', label=\"Class 0 - Correct\")\n",
    "axis.plot(X[ind_10_map_flat_list, 0], X[ind_10_map_flat_list, 1], X[ind_10_map_flat_list, 2],  \n",
    "          'or', label=\"Class 0 - Incorrect\")\n",
    "axis.plot(X[ind_01_map_flat_list, 0], X[ind_01_map_flat_list, 1], X[ind_01_map_flat_list, 2],  \n",
    "          '^r', label=\"Class 1 - Incorrect\")\n",
    "axis.plot(X[ind_11_map_flat_list, 0], X[ind_11_map_flat_list, 1], X[ind_11_map_flat_list, 2],  \n",
    "          '^g', label=\"Class 1 - Correct\")\n",
    "\n",
    "plt.legend()\n",
    "axis.set_xlabel(r\"$x_1$\")\n",
    "axis.set_ylabel(r\"$x_2$\")\n",
    "axis.set_zticks([-4, -2, 0, 2, 4])\n",
    "axis.set_zlabel(r\"$x_3$\")\n",
    "plt.title(\"MAP Decisions (RED incorrect)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14ac50",
   "metadata": {},
   "source": [
    "**PART-A:Question-2, 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import float_info # Threshold smallest positive floating value\n",
    "\n",
    "# Generate ROC curve samples\n",
    "def estimate_roc(discriminant_score, label):\n",
    "    Nlabels = np.array((sum(label == 0), sum(label == 1)))\n",
    "\n",
    "    # Sorting necessary so the resulting FPR and TPR axes plot threshold probabilities in order as a line\n",
    "    sorted_score = sorted(discriminant_score)\n",
    "\n",
    "    # Use gamma values that will account for every possible classification split\n",
    "    gammas = ([sorted_score[0] - float_info.epsilon] +\n",
    "              sorted_score +\n",
    "              [sorted_score[-1] + float_info.epsilon])\n",
    "\n",
    "    # Calculate the decision label for each observation for each gamma\n",
    "    decisions = [discriminant_score >= g for g in gammas]\n",
    "\n",
    "    ind10 = [np.argwhere((d==1) & (label==0)) for d in decisions]\n",
    "    p10 = [len(inds)/Nlabels[0] for inds in ind10]\n",
    "    ind11 = [np.argwhere((d==1) & (label==1)) for d in decisions]\n",
    "    p11 = [len(inds)/Nlabels[1] for inds in ind11]\n",
    "\n",
    "    # ROC has FPR on the x-axis and TPR on the y-axis\n",
    "    roc = np.array((p10, p11))\n",
    "\n",
    "    return roc, gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e8ce9",
   "metadata": {},
   "source": [
    "**Construct the ROC for ERM by changing log(gamma)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c562c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the ROC for ERM by changing log(gamma)\n",
    "roc_erm, _ = estimate_roc(discriminant_score_erm, labels)\n",
    "roc_map = np.array((p_10_map, p_11_map))\n",
    "\n",
    "plt.ioff() # These are Jupyter only lines to avoid showing the figure when I don't want\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(10, 10))\n",
    "plt.ion() # Re-activate \"interactive\" mode\n",
    "\n",
    "ax_roc.plot(roc_erm[0], roc_erm[1])\n",
    "ax_roc.plot(roc_map[0], roc_map[1], 'rx', label=\"Minimum Pr(error) MAP\", markersize=29)\n",
    "ax_roc.legend()\n",
    "ax_roc.set_xlabel(r\"Probability of false alarm $p(D=1|L=0)$\")\n",
    "ax_roc.set_ylabel(r\"Probability of correct decision $p(D=1|L=1)$\")\n",
    "plt.grid(True)\n",
    "display(fig_roc) # Display as .png\n",
    "\n",
    "fig_roc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92664b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For ERM with correct distribution data, the Minimum Pr(error)\",format(prob_error_erm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58e6db",
   "metadata": {},
   "source": [
    "**PART-B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df047165",
   "metadata": {},
   "source": [
    "Given:\n",
    "1. Class conditional pdfs are both Gaussian with the correct True means - Same mean vectors = same $\\mu$\n",
    "2. Mean vectors:\n",
    "    1. $$ \\mu_{0} = \\begin{bmatrix} -1/2 \\\\ -1/2 \\\\ -1/2 \\end{bmatrix}$$\n",
    "    2. $$ \\mu_{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$$\n",
    "3. Class conditional pdfs are both Gaussian with the incorrect covariance matrices = Indetity Matrices\n",
    "4. Covariance Matrices:\n",
    "    1. $$ \\Sigma_{0} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "    2. $$ \\Sigma_{1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_Identity = np.array([[[1, 0, 0],\n",
    "                   [0, 1, 0],\n",
    "                   [0, 0, 1]],\n",
    "                  [[1, 0, 0],\n",
    "                   [0, 1, 0], \n",
    "                   [0, 0, 1]]])  # Gaussian distributions covariance matrices\n",
    "\n",
    "# Expected Risk Minimization Classifier (using true model parameters)\n",
    "class_conditional_likelihoods = np.array([multivariate_normal.pdf(X, mu[i], Sigma_Identity[i]) for i in L])\n",
    "# Discriminant score is LHS of likelihood-ratio test (LRT) \n",
    "discriminant_score_erm = np.log(class_conditional_likelihoods[1]) - np.log(class_conditional_likelihoods[0])\n",
    "\n",
    "# Gamma threshold for MAP decision rule (same gamma on priors only; 0-1 loss simplification)\n",
    "gamma_map = priors[0] / priors[1]\n",
    "decisions_map = discriminant_score_erm >= np.log(gamma_map)\n",
    "\n",
    "# Get indices and probability estimates of the four decision scenarios:\n",
    "# (true negative, false positive, false negative, true positive)\n",
    "\n",
    "# True Negative Probability\n",
    "ind_00_map = np.argwhere((decisions_map==0) & (labels==0))\n",
    "p_00_map = len(ind_00_map) / Nl[0]\n",
    "# False Positive Probability\n",
    "ind_10_map = np.argwhere((decisions_map==1) & (labels==0))\n",
    "p_10_map = len(ind_10_map) / Nl[0]\n",
    "# False Negative Probability\n",
    "ind_01_map = np.argwhere((decisions_map==0) & (labels==1))\n",
    "p_01_map = len(ind_01_map) / Nl[1]\n",
    "# True Positive Probability\n",
    "ind_11_map = np.argwhere((decisions_map==1) & (labels==1))\n",
    "p_11_map = len(ind_11_map) / Nl[1]\n",
    "\n",
    "# Probability of error for MAP classifier, empirically estimated\n",
    "prob_error_erm = np.array((p_10_map, p_01_map)).dot(Nl.T / N)\n",
    "print(\"Smallest Pr(error) for ERM = {}\".format(prob_error_erm))\n",
    "\n",
    "#Code to flatten the list of lists and storing using numpy array in order to plot in 3D.\n",
    "flat_list =  [item for sublist in ind_00_map for item in sublist]\n",
    "ind_00_map_flat_list = np.array(flat_list)\n",
    "\n",
    "flat_list =  [item for sublist in ind_10_map for item in sublist]\n",
    "ind_10_map_flat_list = np.array(flat_list)\n",
    "\n",
    "flat_list =  [item for sublist in ind_01_map for item in sublist]\n",
    "ind_01_map_flat_list = np.array(flat_list)\n",
    "\n",
    "flat_list =  [item for sublist in ind_11_map for item in sublist]\n",
    "ind_11_map_flat_list = np.array(flat_list)\n",
    "\n",
    "\n",
    "# Plotting in 3D and displaying the MAP Decisions.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "axis = plt.axes(projection='3d')\n",
    "axis.plot(X[ind_00_map_flat_list, 0], X[ind_00_map_flat_list, 1], X[ind_00_map_flat_list, 2],  \n",
    "          'og', label=\"Class 0 - Correct\")\n",
    "axis.plot(X[ind_10_map_flat_list, 0], X[ind_10_map_flat_list, 1], X[ind_10_map_flat_list, 2],  \n",
    "          'or', label=\"Class 0 - Incorrect\")\n",
    "axis.plot(X[ind_01_map_flat_list, 0], X[ind_01_map_flat_list, 1], X[ind_01_map_flat_list, 2],  \n",
    "          '^r', label=\"Class 1 - Incorrect\")\n",
    "axis.plot(X[ind_11_map_flat_list, 0], X[ind_11_map_flat_list, 1], X[ind_11_map_flat_list, 2],  \n",
    "          '^g', label=\"Class 1 - Correct\")\n",
    "\n",
    "plt.legend()\n",
    "axis.set_xlabel(r\"$x_1$\")\n",
    "axis.set_ylabel(r\"$x_2$\")\n",
    "axis.set_zticks([-4, -2, 0, 2, 4])\n",
    "axis.set_zlabel(r\"$x_3$\")\n",
    "plt.title(\"MAP Decisions (RED incorrect)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import float_info # Threshold smallest positive floating value\n",
    "\n",
    "# Generate ROC curve samples\n",
    "def estimate_roc(discriminant_score, label):\n",
    "    Nlabels = np.array((sum(label == 0), sum(label == 1)))\n",
    "\n",
    "    # Sorting necessary so the resulting FPR and TPR axes plot threshold probabilities in order as a line\n",
    "    sorted_score = sorted(discriminant_score)\n",
    "\n",
    "    # Use gamma values that will account for every possible classification split\n",
    "    gammas = ([sorted_score[0] - float_info.epsilon] +\n",
    "              sorted_score +\n",
    "              [sorted_score[-1] + float_info.epsilon])\n",
    "\n",
    "    # Calculate the decision label for each observation for each gamma\n",
    "    decisions = [discriminant_score >= g for g in gammas]\n",
    "\n",
    "    ind10 = [np.argwhere((d==1) & (label==0)) for d in decisions]\n",
    "    p10 = [len(inds)/Nlabels[0] for inds in ind10]\n",
    "    ind11 = [np.argwhere((d==1) & (label==1)) for d in decisions]\n",
    "    p11 = [len(inds)/Nlabels[1] for inds in ind11]\n",
    "\n",
    "    # ROC has FPR on the x-axis and TPR on the y-axis\n",
    "    roc = np.array((p10, p11))\n",
    "\n",
    "    return roc, gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the ROC for ERM by changing log(gamma)\n",
    "roc_erm, _ = estimate_roc(discriminant_score_erm, labels)\n",
    "roc_map = np.array((p_10_map, p_11_map))\n",
    "\n",
    "plt.ioff() # These are Jupyter only lines to avoid showing the figure when I don't want\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(10, 10))\n",
    "plt.ion() # Re-activate \"interactive\" mode\n",
    "\n",
    "ax_roc.plot(roc_erm[0], roc_erm[1])\n",
    "ax_roc.plot(roc_map[0], roc_map[1], 'rx', label=\"Minimum Pr(error) MAP using Naive Bayesian Approach\", markersize=29)\n",
    "ax_roc.legend()\n",
    "ax_roc.set_xlabel(r\"Probability of false alarm $p(D=1|L=0)$\")\n",
    "ax_roc.set_ylabel(r\"Probability of correct decision $p(D=1|L=1)$\")\n",
    "plt.grid(True)\n",
    "display(fig_roc) # Display as .png\n",
    "\n",
    "fig_roc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For ERM with incorrect distribution data using Naive bayes approach, the Minimum Pr(error)\",format(prob_error_erm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305da880",
   "metadata": {},
   "source": [
    "**PART-C Fisher's Linear Discriminant Analysis (LDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042308ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lda(X, mu, Sigma, C=2):\n",
    "    \"\"\"  Fisher's Linear Discriminant Analysis (LDA) on data from two classes (C=2).\n",
    "\n",
    "    In practice the mean and covariance parameters would be estimated from training samples.\n",
    "    \n",
    "    Args:\n",
    "        X: Real-valued matrix of samples with shape [N, n], N for sample count and n for dimensionality.\n",
    "        mu: Mean vector [C, n].\n",
    "        Sigma: Covariance matrices [C, n, n].\n",
    "\n",
    "    Returns:\n",
    "        w: Fisher's LDA project vector, shape [n, 1].\n",
    "        z: Scalar LDA projections of input samples, shape [N, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    mu = np.array([mu[i].reshape(-1, 1) for i in range(C)])\n",
    "    cov = np.array([Sigma[i].T for i in range(C)])\n",
    "\n",
    "    # Determine between class and within class scatter matrix\n",
    "    Sb = (mu[1] - mu[0]).dot((mu[1] - mu[0]).T)\n",
    "    Sw = cov[0] + cov[1]\n",
    "\n",
    "    # Regular eigenvector problem for matrix Sw^-1 Sb\n",
    "    lambdas, U = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))\n",
    "    # Get the indices from sorting lambdas in order of increasing value, with ::-1 slicing to then reverse order\n",
    "    idx = lambdas.argsort()[::-1]\n",
    "\n",
    "    # Extract corresponding sorted eigenvectors\n",
    "    U = U[:, idx]\n",
    "\n",
    "    # First eigenvector is now associated with the maximum eigenvalue, mean it is our LDA solution weight vector\n",
    "    w = U[:, 0]\n",
    "\n",
    "    # Scalar LDA projections in matrix form\n",
    "    z = X.dot(w)\n",
    "\n",
    "    return w, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c9875",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fisher LDA Classifer (using true model parameters)\n",
    "weight, discriminant_score_lda = perform_lda(X, mu, Sigma)\n",
    "\n",
    "# Estimate the ROC curve for this LDA classifier\n",
    "roc_lda, gamma_lda = estimate_roc(discriminant_score_lda, labels)\n",
    "\n",
    "# ROC returns FPR vs TPR, but prob error needs FNR so take 1-TPR\n",
    "prob_error_lda = np.array((roc_lda[0,:], 1 - roc_lda[1,:])).T.dot(Nl.T / N)\n",
    "\n",
    "# Min prob error\n",
    "min_prob_error_lda = np.min(prob_error_lda)\n",
    "min_ind = np.argmin(prob_error_lda)\n",
    "\n",
    "# Display the estimated ROC curve for LDA and indicate the operating points\n",
    "# with smallest empirical error probability estimates (could be multiple)\n",
    "ax_roc.plot(roc_lda[0], roc_lda[1], 'b:')\n",
    "ax_roc.plot(roc_lda[0, min_ind], roc_lda[1, min_ind], 'r.', label=\"Minimum Pr(error) LDA\", markersize=16)\n",
    "ax_roc.set_title(\"ROC Curves for ERM and LDA\")\n",
    "ax_roc.legend()\n",
    "\n",
    "plt.show()\n",
    "display(fig_roc)\n",
    "fig_roc;\n",
    "\n",
    "print(\"Weight Vector\", weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use min-error threshold\n",
    "decisions_lda = discriminant_score_lda >= gamma_lda[min_ind]\n",
    "\n",
    "# Get indices and probability estimates of the four decision scenarios:\n",
    "# (true negative, false positive, false negative, true positive)\n",
    "\n",
    "# True Negative Probability\n",
    "ind_00_lda = np.argwhere((decisions_lda==0) & (labels==0))\n",
    "p_00_lda = len(ind_00_lda) / Nl[0]\n",
    "# False Positive Probability\n",
    "ind_10_lda = np.argwhere((decisions_lda==1) & (labels==0))\n",
    "p_10_lda = len(ind_10_lda) / Nl[0]\n",
    "# False Negative Probability\n",
    "ind_01_lda = np.argwhere((decisions_lda==0) & (labels==1))\n",
    "p_01_lda = len(ind_01_lda) / Nl[1]\n",
    "# True Positive Probability\n",
    "ind_11_lda = np.argwhere((decisions_lda==1) & (labels==1))\n",
    "p_11_lda = len(ind_11_lda) / Nl[1]\n",
    "\n",
    "#Code to flatten the list of lists and storing using numpy array in order to plot in 3D.\n",
    "flat_list =  [item for sublist in ind_00_lda for item in sublist]\n",
    "ind_00_lda_flat_list = np.array(flat_list).astype(int)\n",
    "\n",
    "flat_list =  [item for sublist in ind_10_lda for item in sublist]\n",
    "ind_10_lda_flat_list = np.array(flat_list).astype(int)\n",
    "\n",
    "flat_list =  [item for sublist in ind_01_lda for item in sublist]\n",
    "ind_01_lda_flat_list = np.array(flat_list).astype(int)\n",
    "\n",
    "flat_list =  [item for sublist in ind_11_lda for item in sublist]\n",
    "ind_11_lda_flat_list = np.array(flat_list).astype(int)\n",
    "\n",
    "\n",
    "# Plotting in 3D and displaying the MAP Decisions.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "axis = plt.axes(projection='3d')\n",
    "axis.plot(X[ind_00_lda_flat_list, 0], X[ind_00_lda_flat_list, 1], X[ind_00_lda_flat_list, 2],  \n",
    "          'og', label=\"Class 0 - Correct\")\n",
    "axis.plot(X[ind_10_lda_flat_list, 0], X[ind_10_lda_flat_list, 1], X[ind_10_lda_flat_list, 2],  \n",
    "          'or', label=\"Class 0 - Incorrect\")\n",
    "axis.plot(X[ind_01_lda_flat_list, 0], X[ind_01_lda_flat_list, 1], X[ind_01_lda_flat_list, 2],  \n",
    "          '^r', label=\"Class 1 - Incorrect\")\n",
    "axis.plot(X[ind_11_lda_flat_list, 0], X[ind_11_lda_flat_list, 1], X[ind_11_lda_flat_list, 2],  \n",
    "          '^g', label=\"Class 1 - Correct\")\n",
    "\n",
    "plt.legend()\n",
    "axis.set_xlabel(r\"$x_1$\")\n",
    "axis.set_ylabel(r\"$x_2$\")\n",
    "axis.set_zticks([-4, -2, 0, 2, 4])\n",
    "axis.set_zlabel(r\"$x_3$\")\n",
    "plt.title(\"LDA Decisions (RED incorrect)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca41e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smallest P(error) for LDA = {}\".format(min_prob_error_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72440b63",
   "metadata": {},
   "source": [
    "Findings:\n",
    "\n",
    "1. For Part-A, The initial implementation of ERM (or MAP) mismatches with the Naive Bayesian. It is clear from the variations in minimum probability of errors. The ROC curves are similar with slight deviation.\n",
    "\n",
    "2. For Part-C, We anticipated that the MAP classifier's error probability would be lower than that of all other classifiers, including Fisher's LDA, because it is built to reduce probability error (when used with correct class conditional likelihoods and class priors). This theoretical result is supported by the numerical experiment.\n",
    "\n",
    "3. We also note that because the dataset is random, it is possible that the empirical error probability estimate of the LDA classifier will occasionally (less frequently for larger N) be lower than that of the MAP classifier. This is because error probability estimates are subject to random sample variations.\n",
    "\n",
    "4. Also take note of how the LDA classifier's ROC curve in this instance resembles both the ERM (MAP) classifier and the Naive Bayesian technique used in the ERM with incorrect data.\n",
    "\n",
    "5. The same steps as in Part A are used to calculate the gammas and minimum error probability. There is only a small discrepancy between these values and the LDA and Naive Bayesian Classifier. The ROC curve doesn't negatively affected by the model mismatch, and both the theoretical and experimental probability of mistakes actually decreased significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db1fd2",
   "metadata": {},
   "source": [
    "**Question-2**\n",
    "**PART-A.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget to manipulate plots in Jupyter notebooks\n",
    "import matplotlib.pyplot as plt # For general plotting\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import multivariate_normal # MVN not univariate\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Set seed to generate reproducible \"pseudo-randomness\" (handles scipy's \"randomness\" too)\n",
    "np.random.seed(7)\n",
    "\n",
    "plt.rc('font', size=22)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=18)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=16)    # legend fontsize\n",
    "plt.rc('figure', titlesize=22)   # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0355cc",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model Setting\n",
    "\n",
    "Mixture models have the form:\n",
    "\n",
    "$$ p(\\mathbf{x}) = \\sum_{c=1}^C a_c p_c(\\mathbf{x}). $$\n",
    "\n",
    "These models are a linear combination of multiple component distributions, where $p_c(\\mathbf{x})$ is the $c^{th}$ probability density function (pdf), and $a_c$ is the $c^{th}$ \"mixture weight\", such that both $0 \\leq a_c \\leq 1$ and $\\sum_{c=1}^C a_c = 1$ are satisfied.\n",
    "\n",
    "Let's look at the concrete example of a Gaussian Mixture Model (GMM), where $p_c(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x} \\,|\\, \\boldsymbol{\\mu}_c, \\boldsymbol{\\Sigma}_c)$ for $c \\in \\{1, ..., C\\}$, with each $c^{th}$ component distribution represented as a multivariate Gaussian. Effectively, we are \"choosing\" a Gaussian pdf to sample from based on the weights $a_c$.\n",
    "\n",
    "Below provides an example of how to draw samples using the SciPy library, notably the `scipy.stats` module and its `multivariate_normal` class. It also defines the dataset setting for our ERM demonstration, which consists of the following conditional log-likelihoods for 4 classes with true labels $L$:\n",
    "\n",
    "$$ p(\\mathbf{x} \\,|\\, L=1) = \\mathcal{N}(\\mathbf{x} \\,|\\, \\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}_{1}),$$\n",
    "$$ p(\\mathbf{x} \\,|\\, L=2) = \\mathcal{N}(\\mathbf{x} \\,|\\, \\boldsymbol{\\mu}_{2}, \\boldsymbol{\\Sigma}_{2}),$$\n",
    "$$ p(\\mathbf{x} \\,|\\, L=3) = \\mathcal{N}(\\mathbf{x} \\,|\\, \\boldsymbol{\\mu}_{3}, \\boldsymbol{\\Sigma}_{3}),$$\n",
    "$$ p(\\mathbf{x} \\,|\\, L=4) = \\mathcal{N}(\\mathbf{x} \\,|\\, \\boldsymbol{\\mu}_{4}, \\boldsymbol{\\Sigma}_{4}),$$\n",
    "\n",
    "\n",
    "Given:\n",
    "1. Class priors as p(L=0) = 0.2, p(L=1) = 0.25, p(L=2) = 0.25, p(L=3) = 0.3\n",
    "2. Mean vectors are equally spaces along the line.\n",
    "3. Covariance matrices are scaled versions of identity matrix.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\boldsymbol{\\mu}_1=\\begin{bmatrix} 1\\\\0 \\end{bmatrix}, ~~~\n",
    "    \\boldsymbol{\\mu}_2=\\begin{bmatrix} 5\\\\0 \\end{bmatrix}, ~~~\n",
    "    \\boldsymbol{\\mu}_3=\\begin{bmatrix} 9\\\\0 \\end{bmatrix}, ~~~\n",
    "    \\boldsymbol{\\mu}_4=\\begin{bmatrix} 13\\\\0 \\end{bmatrix} ~~~ \\\\\n",
    "    \\boldsymbol{\\Sigma}_1=\\begin{bmatrix}1 & 0 \\\\0 & 1 \\end{bmatrix}, ~~~\n",
    "    \\boldsymbol{\\Sigma}_2=\\begin{bmatrix}2 & 0 \\\\0 & 2 \\end{bmatrix}, ~~~\n",
    "    \\boldsymbol{\\Sigma}_3=\\begin{bmatrix}3 & 0 \\\\0 & 3 \\end{bmatrix},\n",
    "    \\boldsymbol{\\Sigma}_4=\\begin{bmatrix}4 & 0 \\\\0 & 4 \\end{bmatrix}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bf1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples to draw from each distribution\n",
    "N = 10000\n",
    "\n",
    "# Likelihood of each distribution to be selected AND class priors!!!\n",
    "priors = np.array([0.2, 0.25, 0.25, 0.3])  \n",
    "# Determine number of classes/mixture components\n",
    "C = len(priors)\n",
    "mu = np.array([[1, 0],\n",
    "               [5, 0],\n",
    "               [9, 0],\n",
    "               [13, 0]])  # Gaussian distributions means\n",
    "Sigma = np.array([[[1, 0],\n",
    "                   [0, 1]],\n",
    "                  [[2, 0],\n",
    "                   [0, 2]],\n",
    "                  [[3, 0],\n",
    "                   [0, 3]],\n",
    "                  [[4, 0],\n",
    "                   [0, 4]]])  # Gaussian distributions covariance matrices\n",
    "\n",
    "# Determine dimensionality from mixture PDF parameters\n",
    "n = mu.shape[1]\n",
    "# Output samples and labels\n",
    "X = np.zeros([N, n])\n",
    "labels = np.zeros(N)\n",
    "\n",
    "# Decide randomly which samples will come from each component u_i ~ Uniform(0, 1) for i = 1, ..., N (or 0, ... , N-1 in code)\n",
    "u = np.random.rand(N)\n",
    "# Determine the thresholds based on the mixture weights/priors for the GMM, which need to sum up to 1\n",
    "thresholds = np.cumsum(priors)\n",
    "\n",
    "for c in range(C):\n",
    "    c_ind = np.argwhere(u <= thresholds[c])[:, 0]  # Get randomly sampled indices for this component\n",
    "    c_N = len(c_ind)  # No. of samples in this component\n",
    "    labels[c_ind] = c * np.ones(c_N)\n",
    "    u[c_ind] = 1.1 * np.ones(c_N)  # Multiply by 1.1 to fail <= thresholds and thus not reuse samples\n",
    "    X[c_ind, :] =  multivariate_normal.rvs(mu[c], Sigma[c], c_N)\n",
    "\n",
    "# Plot the original data and their true labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(X[labels==0, 0], X[labels==0, 1], 'bo', label=\"Class 1\")\n",
    "plt.plot(X[labels==1, 0], X[labels==1, 1], 'rx', label=\"Class 2\");\n",
    "plt.plot(X[labels==2, 0], X[labels==2, 1], 'g+', label=\"Class 3\");\n",
    "plt.plot(X[labels==3, 0], X[labels==3, 1], 'y^', label=\"Class 4\");\n",
    "plt.legend()\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.title(\"Data and True Labels\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61532ed7",
   "metadata": {},
   "source": [
    "**PART-A.2 and PART-A.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb119e97",
   "metadata": {},
   "source": [
    "## ERM Classification using True Knowledge of PDF\n",
    "\n",
    "For this demo of ERM, we are specifically going to use <b>0-1 loss</b>, such that the decision rule we wish to derive achieves minimum probability of error, i.e. the <b> maximum a posteriori (MAP)</b> classification rule. We will implement this classifier with the true distribution knowledge we have outlined above, and count samples per decision-label pair to produce a confusion matrix with entries $p(D=i\\,|\\,L=j)$ for $i,j \\in \\{1, 2, 3, 4\\}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ff2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.array(range(C))  # 0-(C-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556470d",
   "metadata": {},
   "source": [
    "The decision rule that achieves minimum probability of error uses a loss matrix $\\mathbf{\\Lambda}$ with entries $\\lambda_{ij} = 1 - \\delta_{ij}$ where $\\lambda_{ij}$ is the loss associated with deciding class label $i$ given $\\mathbf{x}$ comes from class label $j$ and $\\delta_{ij}$ is the Kronecker delta: \n",
    "\n",
    "$$\\lambda_{ij} =\n",
    "    \\begin{cases}\n",
    "        0  & \\text{if} \\; i = j \\\\\n",
    "         1 & \\text{if} \\; i \\neq j\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "From this, we can define the following 0-1 loss matrix:\n",
    "\n",
    "$$ \\mathbf{\\Lambda} = \\begin{bmatrix} 0 & 1 & 1 & 1 \\\\ 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 &1 \\\\ 1 & 1 & 1 &0 \\end{bmatrix}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use a 0-1 loss matrix for this problem\n",
    "Lambda = np.ones((C, C)) - np.identity(C)\n",
    "print(Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37faa605",
   "metadata": {},
   "source": [
    "For a given $\\mathbf{x}$, we want to choose a class label $i$ which minimizes risk (or loss) associated with choosing this class label. Hence, we need a decision rule $D(\\mathbf{x})$, which is a function $D : \\mathbb{R}^n \\rightarrow {1, \\ldots, C} \\in \\mathbb{Z}$ that tells us which action to take or decision to make for every possible observation.\n",
    "\n",
    "We know that the ERM decision rule $D(\\mathbf{x})$ is based on minimizing conditional risk, defined as:\n",
    "\n",
    "$$ D(\\mathbf{x}) = \\mathop{\\rm argmin}_{i\\in \\{1, 2, 3, 4\\}} R(D = i \\, | \\, \\mathbf{x}) = \\mathop{\\rm argmin}_{i\\in \\{1, 2, 3, 4\\}} \\sum_{j=1}^4 \\lambda_{ij} p(L = j \\, | \\, \\mathbf{x}).$$\n",
    "\n",
    "In the case of a 0-1 loss function $\\lambda_{ij}$, then:\n",
    "\n",
    "\\begin{align*}\n",
    "R(D = i \\, | \\, \\mathbf{x}) \n",
    "& = \\sum_{j=1}^4 \\lambda_{ij} p(L = j \\, | \\, \\mathbf{x}) \\\\\n",
    "& = \\sum_{j \\neq i} p(L = j \\, | \\, \\mathbf{x}) \\\\\n",
    "& = 1 - p(L = i \\, | \\, \\mathbf{x}).\n",
    "\\end{align*}\n",
    "\n",
    "As a result, the decision rule $D(\\mathbf{x})$ ends up <b>minimizing the average probability of error</b> (misclassification rate or posterior expected loss):\n",
    "\n",
    "\\begin{align*}\n",
    " D(\\mathbf{x}) & = \\mathop{\\rm argmin}_{i\\in \\{1, 2, 3, 4\\}} R(D = i \\, | \\, \\mathbf{x}) \\\\ \n",
    "               & = \\mathop{\\rm argmin}_{i\\in \\{1, 2, 3, 4\\}} 1 - p(L = i \\, | \\, \\mathbf{x}) \\\\\n",
    "               & = \\mathop{\\rm argmax}_{i\\in \\{1, 2, 3, 4\\}} p(L = i \\, | \\, \\mathbf{x}),\n",
    "\\end{align*}\n",
    "\n",
    "or equivalently, as shown in the last step, computing the <b>MAP estimate</b>.\n",
    "\n",
    "While we have just derived the ERM decision rule <i>without</i> needing a loss matrix $\\boldsymbol{\\Lambda}$ due to the 0-1 loss setting, I will still proceed with its application for sake of generality.\n",
    "\n",
    "In particular, I wish to break down $D(\\mathbf{x})$ in the code by first converting the risk $R(D = i \\, | \\, \\mathbf{x})$ of choosing decision $i$ given observation $\\mathbf{x}$ into matrix form (see extra notes):\n",
    "\n",
    "$$ \\begin{bmatrix} R(D=1\\, | \\,\\mathbf{x}) \\\\  R(D=2\\, | \\,\\mathbf{x}) \\\\ R(D=3\\, | \\,\\mathbf{x} \\\\  R(D=4\\, | \\,\\mathbf{x}) \\end{bmatrix} = \\mathbf{\\Lambda} \\begin{bmatrix} p(L=1\\, | \\,\\mathbf{x}) \\\\ p(L=2\\, | \\,\\mathbf{x}) \\\\ p(L=3\\, | \\,\\mathbf{x} \\\\ p(L=4\\, | \\,\\mathbf{x}) \\end{bmatrix} = \\mathbf{\\Lambda} \\, \\text{diag}\\big(p(L=1), p(L=2), p(L=3), p(L=4) \\big) \\begin{bmatrix} p(\\mathbf{x}\\, | \\,L=1) \\\\ p(\\mathbf{x}\\, | \\,L=2) \\\\ p(\\mathbf{x}\\, | \\,L=3) \\\\ p(\\mathbf{x}\\, | \\,L=4) \\end{bmatrix}.$$\n",
    "\n",
    "The LHS of this equality is the risk across all decision options, and the RHS of the equality is the loss matrix and class posteriors, which have been expanded using Bayes rule into likelihood and prior terms. Therefore we can proceed in the code by computing the class posteriors from the priors and conditional likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2efa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class-conditional likelihoods p(x|L=j) for each label of the N observations\n",
    "class_cond_likelihoods = np.array([multivariate_normal.pdf(X, mu[j], Sigma[j]) for j in L])\n",
    "class_priors = np.diag(priors)\n",
    "print(\"Class condition likelihoods\",class_cond_likelihoods.shape)\n",
    "print(\"Clas priors\", class_priors.shape)\n",
    "# Matrix of posteriors, shaped [C, N]\n",
    "class_posteriors = class_priors.dot(class_cond_likelihoods)\n",
    "print(\"Class posteriors\", class_posteriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f2c7f",
   "metadata": {},
   "source": [
    "And then the conditional risk scores using $\\boldsymbol{\\Lambda}$, which again I emphasize is not required for 0-1 loss but demonstrates the full ERM framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create the risk matrix of size C x N\n",
    "cond_risk = Lambda.dot(class_posteriors)\n",
    "print(\"Condition Risk\", cond_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105579e6",
   "metadata": {},
   "source": [
    "This finally leaves us with our decision vector for all observations:\n",
    "\n",
    "$$ D(\\mathbf{x}) = \\mathop{\\rm argmin}_{i\\in \\{1, 2, 3, 4\\}} R(D = i \\, | \\, \\mathbf{x}). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the decision for each column in risk_mat\n",
    "decisions = np.argmin(cond_risk, axis=0)\n",
    "print(\"Decisions\", decisions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6baf5",
   "metadata": {},
   "source": [
    "We can now yield the following <b>normalized</b> confusion matrix over the true columns (class sample counts):\n",
    "    \n",
    "$$ \\begin{bmatrix} p(D=1\\, | \\,L=1) & \\cdots & p(D=4\\, | \\,L=1) \\\\ \\vdots & \\ddots & \\vdots \\\\  p(D=1\\, | \\,L=4) & \\cdots & p(D=4\\, | \\,L=4)  \\end{bmatrix}. $$\n",
    "\n",
    "And reminder that the <b>probability of error</b> is:\n",
    "\n",
    "$$ \\text{Pr}(\\text{error}) = 1 - \\text{Pr}(\\text{correct}) = 1 - \\frac{1}{N}\\sum_i^4 N_i p(D = i \\,|\\, L = i),$$\n",
    "\n",
    "with $N_i$ as the number of samples belonging to class $i$. So the final minimum probability of error estimate using our MAP classifier is computed by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45298c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for decisions vs true labels\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "marker_shapes = 'o.^*' # Accomodates up to C=5\n",
    "marker_colors = 'gr'\n",
    "\n",
    "# Get sample class counts\n",
    "sample_class_counts = np.array([sum(labels == j) for j in L])\n",
    "\n",
    "# Confusion matrix compute from scratch\n",
    "conf_mat = np.zeros((C, C))\n",
    "for i in L: # Each decision option\n",
    "    for j in L: # Each class label\n",
    "        ind_ij = np.argwhere((decisions==i) & (labels==j))\n",
    "        conf_mat[i, j] = len(ind_ij) / sample_class_counts[j] # Average over class sample count\n",
    "\n",
    "        # True label = Marker shape; Decision = Marker Color\n",
    "        marker = marker_shapes[j]\n",
    "        \n",
    "        \n",
    "\n",
    "        if i != j:\n",
    "            plt.plot(X[ind_ij, 0], X[ind_ij, 1], marker+'r')\n",
    "        else:\n",
    "            plt.plot(X[ind_ij, 0], X[ind_ij, 1], marker+'g')\n",
    "            \n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "# # Can also compute the normalized confusion matrix from scikit-learn's method\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# print(\"Normalized Sklearn Confusion Matrix (column basis):\")\n",
    "# conf_mat = confusion_matrix(decisions, labels, normalize='pred')\n",
    "# print(conf_mat)\n",
    "\n",
    "prob_error = 1 - np.diag(conf_mat).dot(sample_class_counts) / N\n",
    "print(\"Minimum Probability of Error: \", prob_error)\n",
    "\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.title(\"Minimum Probability of Error:  {:.3f}\".format(prob_error))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af7ca0",
   "metadata": {},
   "source": [
    "**Question-2:Part-B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baca22b",
   "metadata": {},
   "source": [
    "$$ \\mathbf{\\Lambda} = \\begin{bmatrix} 0 & 1 & 2 & 3 \\\\ 1 & 0 & 1 & 2 \\\\ 2 & 1 & 0 &1 \\\\ 3 & 2 & 1 &0 \\end{bmatrix}. $$\n",
    "Repating the same with the above loss values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4287d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use a 0-1 loss matrix for this problem\n",
    "Lambda = np.array([[0.0,1.0,2.0,3.0],[1.0,0.0,1.0,2.0],[2.0,1.0,0.0,1.0],[3.0,2.0,1.0,0.0]])\n",
    "print(\"Lambda\",Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class-conditional likelihoods p(x|L=j) for each label of the N observations\n",
    "class_cond_likelihoods = np.array([multivariate_normal.pdf(X, mu[j], Sigma[j]) for j in L])\n",
    "class_priors = np.diag(priors)\n",
    "print(\"class_cond_likelihoods\", class_cond_likelihoods.shape)\n",
    "print(\"class_priors\", class_priors.shape)\n",
    "# Matrix of posteriors, shaped [C, N]\n",
    "class_posteriors = class_priors.dot(class_cond_likelihoods)\n",
    "print(\"class_posteriors\", class_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4571d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create the risk matrix of size C x N\n",
    "cond_risk = Lambda.dot(class_posteriors)\n",
    "print(\"cond_risk\", cond_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the decision for each column in risk_mat\n",
    "decisions_new = np.argmin(cond_risk, axis=0)\n",
    "print(\"decisions_new\", decisions_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for decisions vs true labels\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "marker_shapes = 'o.^*' # Accomodates up to C=5\n",
    "marker_colors = 'gr'\n",
    "\n",
    "# Get sample class counts\n",
    "sample_class_counts = np.array([sum(labels == j) for j in L])\n",
    "\n",
    "# Confusion matrix compute from scratch\n",
    "conf_mat = np.zeros((C, C))\n",
    "for i in L: # Each decision option\n",
    "    for j in L: # Each class label\n",
    "        ind_ij = np.argwhere((decisions_new==i) & (labels==j))\n",
    "        conf_mat[i, j] = len(ind_ij) / sample_class_counts[j] # Average over class sample count\n",
    "\n",
    "        # True label = Marker shape; Decision = Marker Color\n",
    "        marker = marker_shapes[j]\n",
    "        \n",
    "        \n",
    "\n",
    "        if i != j:\n",
    "            plt.plot(X[ind_ij, 0], X[ind_ij, 1], marker+'r')\n",
    "        else:\n",
    "            plt.plot(X[ind_ij, 0], X[ind_ij, 1], marker+'g')\n",
    "            \n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "# # Can also compute the normalized confusion matrix from scikit-learn's method\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# print(\"Normalized Sklearn Confusion Matrix (column basis):\")\n",
    "# conf_mat = confusion_matrix(decisions, labels, normalize='pred')\n",
    "# print(conf_mat)\n",
    "\n",
    "prob_error = 1 - np.diag(conf_mat).dot(sample_class_counts) / N\n",
    "print(\"Minimum Probability of Error: \", prob_error)\n",
    "\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.title(\"Minimum Probability of Error:  {:.3f}\".format(prob_error))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b62b1",
   "metadata": {},
   "source": [
    "Findings:\n",
    "\n",
    "1. It can be seen that certain statistics and charts, such as the confusion matrix and probability error score, are not altering considerably after analysis. It might have occurred because the loss matrices for parts a and b were symmetric.\n",
    "\n",
    "2. When compared to class 1 and 2, points with labels 3 and 4 have more wrongly anticipated points after plot observation.\n",
    "\n",
    "3. Minimum probablity error for Part-A is 0.15259999999999996 and Part-B is 0.15300000000000002 which is very less.\n",
    "\n",
    "4. Even in the confusion matrix, the values for the two portions are nearly identical with only a slight variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4cc33",
   "metadata": {},
   "source": [
    "**Question-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c721ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cov_mu(data):\n",
    "    '''\n",
    "    Estimates the covariance matrix and mean vector for all the true class\n",
    "    labels in the input data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.DataFrame\n",
    "    Given data set\n",
    "    Returns\n",
    "    -------\n",
    "    data_info: pandas.DataFrame\n",
    "    The true labels, covariance matrics and mean vectors\n",
    "    '''\n",
    "    true_labels = data.index.unique().tolist()\n",
    "    data_info   = pd.DataFrame(columns=['True Class Label', 'Covariance Matrix', 'Mean Vector', 'Number of Samples', 'Class Prior'])\n",
    "    total_samples = 0\n",
    "    for true_label in true_labels:\n",
    "        temp = data.loc[true_label, :]\n",
    "        #cov = np.cov(temp, bias=True)\n",
    "        cov = temp.cov().to_numpy()\n",
    "        mean = temp.mean(axis=0).tolist()\n",
    "        n = temp.shape[0]\n",
    "        total_samples = total_samples + n\n",
    "        d = {'True Class Label': true_label, 'Covariance Matrix': cov, 'Mean Vector': mean, 'Number of Samples': n}\n",
    "        data_info = data_info.append(d, ignore_index=True)\n",
    "    data_info['Class Prior'] = data_info['Number of Samples'] / total_samples\n",
    "    return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7de5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset(data, subset=['x','y','z']):\n",
    "    '''\n",
    "    Plots the four-dimensions of the samples taken from the distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.DataFrame\n",
    "    Contains the sample data\n",
    "    subset: array, optional\n",
    "    Plots these three values.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    markers = ['v', '^', '<', '>', '8', 's', 'p', '*', 'h', '+', 'x', 'D']\n",
    "    fig = plt.figure(figsize = (5.5,5))\n",
    "    fig.subplots_adjust(left=0.01, right=0.96, top=0.99, bottom=0.01, wspace=0)\n",
    "    ax = plt.axes(projection =\"3d\")\n",
    "    true_labels = data.sort_index().index.unique().tolist()\n",
    "    for i, true_label in enumerate(true_labels):\n",
    "        temp = data.loc[true_label, :]\n",
    "        xs = temp[subset[0]].tolist()\n",
    "        ys = temp[subset[1]].tolist()\n",
    "        zs = temp[subset[2]].tolist()\n",
    "        ax.scatter3D(xs, ys, zs, label=true_label, marker=markers[i], alpha=0.3)\n",
    "    ax.set_xlabel('%s'%subset[0])\n",
    "    ax.set_ylabel('%s'%subset[1])\n",
    "    ax.set_zlabel('%s'%subset[2])\n",
    "    ax.legend(loc='upper left', title='Class Label')\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig('./%s_%s_%s_true_classes.pdf'%(subset[0], subset[1], subset[2]))\n",
    "    plt.clf()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decisions(data, data_info, loss_matrix=None):\n",
    "    '''\n",
    "    Implement classifier and check if correct given the true\n",
    "    data dsitribution knowledge. Chooses minimum risk.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.DataFrame\n",
    "    Contains the sample data\n",
    "    data_info: pandas.DataFrame\n",
    "    loss_matrix: array\n",
    "    2d array = lambda\n",
    "    Returns\n",
    "    -------\n",
    "    data: pandas.DataFrame\n",
    "    modified data with classification and accuracy info.\n",
    "    '''\n",
    "    choices = []\n",
    "    correct = []\n",
    "    dimension_labels = data.columns.tolist()\n",
    "    class_labels = data.sort_index().index.unique().tolist()\n",
    "    # Create 0-1 loss matrix if none is given\n",
    "    if(loss_matrix==None):\n",
    "        d = max(class_labels)\n",
    "        loss_matrix = np.zeros((d,d))\n",
    "        for i in range(0,d):\n",
    "            for j in range(0,d):\n",
    "                if(i==j):\n",
    "                    loss_matrix[i][j] = 0\n",
    "                else:\n",
    "                    loss_matrix[i][j] = 1\n",
    "    print(loss_matrix)\n",
    "    labels_reference = {i:class_labels[i] for i in range(0,len(class_labels))}\n",
    "    for idx, row in data.iterrows():\n",
    "        # Modify class label for computation\n",
    "        distribution = int(row.name)\n",
    "        rows = [row[dimension_label] for dimension_label in dimension_labels]\n",
    "        #print(rows)\n",
    "        #print(class_labels)\n",
    "        args = [risk(class_label-1, rows, loss_matrix, data_info) for class_label in class_labels]\n",
    "        choice = labels_reference[np.argmin(args)]\n",
    "        choices.append(choice)\n",
    "        #print('Choice: %d'%choice)\n",
    "        #print('Correct: %d'%distribution)\n",
    "        # Check if classification was correct or not\n",
    "        if(choice==distribution):\n",
    "            correct.append(True)\n",
    "        #print('Correct!: %d'%len(correct))\n",
    "        else:\n",
    "            correct.append(False)\n",
    "    data['ERM Classification'] = choices\n",
    "    data['Correct'] = correct\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fcfe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk(i , x , loss_matrix, data_info):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_info: pandas.DataFrame\n",
    "        Info on true classes in distributions.\n",
    "    i: int\n",
    "        The true class assigned to i\n",
    "    x: \n",
    "    p: float32\n",
    "        The class prior\n",
    "    loss_matrix: array, optional\n",
    "    '''\n",
    "    risk = 0\n",
    "    for j, row in data_info.iterrows():\n",
    "        #  Probability, mu, sigma^2\n",
    "        try:\n",
    "            #print(x)\n",
    "            risk = risk + loss_matrix[i][int(row['True Class Label'])-1]*row['Class Prior']*multivariate_normal.pdf(x,row['Mean Vector'],row['Covariance Matrix'])\n",
    "            #print(risk)\n",
    "        except np.linalg.LinAlgError:\n",
    "            continue\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_matrix(data, save_path='./decision_matrix.pdf'):\n",
    "    '''\n",
    "    Plots a heatmap of the decision matrix along with the values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas.DataFrame\n",
    "        Contains the sample data\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    pred = data['ERM Classification'].tolist()\n",
    "    act  = data.index.tolist()\n",
    "    class_labels = data.sort_index().index.unique().tolist()\n",
    "    confusion = confusion_matrix(act, pred, normalize='true')\n",
    "    sns.heatmap(data=confusion,cmap=\"YlOrRd\",annot=True, xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Decision')\n",
    "    plt.ylabel('True Class Label')\n",
    "    positions = range(0,len(class_labels))\n",
    "    #plt.xticks(positions, class_labels)\n",
    "    #plt.yticks(positions, class_labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.clf()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correct_classified(data, subset=['x','y','z']):\n",
    "    '''\n",
    "    Plots the four-dimensions of the samples taken from the distribution and if the classification was correct.\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples_path: string\n",
    "        File containing the sample data\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    markers = ['v', '^', '<', '>', '8', 's', 'p', '*', 'h', '+', 'x', 'D']\n",
    "    fig = plt.figure(figsize = (5.5,5))\n",
    "    fig.subplots_adjust(left=0.01, right=0.96, top=0.99, bottom=0.01, wspace=0)\n",
    "    ax = plt.axes(projection =\"3d\")\n",
    "    # Plot correct\n",
    "    correct = data[data['Correct']==True]\n",
    "    true_labels = correct.sort_index().index.unique().tolist()\n",
    "    print('Number of correct classified points: %d'%correct.shape[0])\n",
    "    for i, true_label in enumerate(true_labels):\n",
    "        temp = correct.loc[true_label, :]\n",
    "        xs = temp[subset[0]].tolist()\n",
    "        ys = temp[subset[1]].tolist()\n",
    "        zs = temp[subset[2]].tolist()\n",
    "        ax.scatter3D(xs, ys, zs, label=true_label, marker=markers[i], alpha=0.3, color='green')\n",
    "    # Plot incorrect\n",
    "    correct = data[data['Correct']==False]\n",
    "    true_labels = correct.sort_index().index.unique().tolist()\n",
    "    print('Number of incorrect classified points: %d'%correct.shape[0])\n",
    "    for i, true_label in enumerate(true_labels):\n",
    "        temp = correct.loc[true_label, :]\n",
    "        xs = temp[subset[0]].tolist()\n",
    "        ys = temp[subset[1]].tolist()\n",
    "        zs = temp[subset[2]].tolist()\n",
    "        ax.scatter3D(xs, ys, zs, label=true_label, marker=markers[i], alpha=0.3, color='red')\n",
    "    ax.set_xlabel('%s'%subset[0])\n",
    "    ax.set_ylabel('%s'%subset[1])\n",
    "    ax.set_zlabel('%s'%subset[2])\n",
    "    #ax.get_legend().remove()\n",
    "    green_patch = mpatches.Patch(color='green', label='Correct')\n",
    "    red_patch = mpatches.Patch(color='red', label='Incorrect')\n",
    "    ax.legend(handles=[green_patch, red_patch], loc='upper left', title='Classification')\n",
    "    plt.savefig('./%s_%s_%s_true_class_classified_loss2.pdf'%(subset[0], subset[1], subset[2]))\n",
    "    plt.clf()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sample_data(samples, save_path):\n",
    "    '''\n",
    "    Saves the sample data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples: numpy.array\n",
    "        The generated sample data\n",
    "    save_path: string\n",
    "        File name and path to save the sample data\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    samples.to_csv(save_path)\n",
    "    \n",
    "def read_sample_data(save_path):\n",
    "    '''\n",
    "    Read the sample data. Helper function to read data in other functions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    save_path: string\n",
    "        File containing the sample data\n",
    "    Returns\n",
    "    -------\n",
    "    samples: pandas.DataFrame\n",
    "        The generated sample data\n",
    "    '''\n",
    "    samples = pd.read_csv(save_path, index_col=0)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07044365",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # Dimensions: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide\n",
    "    wine_path = 'winequality-white.csv'\n",
    "    wine_df = pd.read_csv(wine_path, delimiter=';', index_col='quality')\n",
    "    print(wine_df)\n",
    "    data_info = estimate_cov_mu(data=wine_df)\n",
    "    print(data_info)\n",
    "    wine_loss_matrix = [[0, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    [15, 0, 10, 15, 20, 25, 30, 35, 40],\n",
    "    [20, 10, 0, 5, 10, 15, 20, 25, 30],\n",
    "    [25, 15, 5, 0, 1, 5, 10, 15, 20],\n",
    "    [30, 20, 10, 1, 0, 1, 1, 5, 10],\n",
    "    [35, 25, 15, 5, 1, 0, 10, 15, 20],\n",
    "    [40, 30, 20, 10, 1, 10, 0, 25, 30],\n",
    "    [45, 35, 25, 15, 5, 15, 25, 0, 40],\n",
    "    [50, 40, 30, 20, 10, 20, 30, 40, 0 ]]\n",
    "    # alcohol, pH, residual sugar\n",
    "    plot_subset(data=wine_df, subset=['alcohol', 'pH', 'residual sugar'])\n",
    "    # citric acid, total sulfer dioxide, density\n",
    "    plot_subset(data=wine_df, subset=['citric acid', 'total sulfur dioxide', 'density'])\n",
    "    wine_df = make_decisions(data=wine_df, data_info=data_info, loss_matrix=wine_loss_matrix)\n",
    "    #print(wine_df)\n",
    "    plot_correct_classified(data=wine_df, subset=['alcohol', 'pH', 'residual sugar'])\n",
    "    plot_correct_classified(data=wine_df, subset=['citric acid', 'total sulfur dioxide', 'density'])\n",
    "    plot_decision_matrix(data=wine_df, save_path='./wine_decision_matrix_loss2.pdf')\n",
    "    x_test = 'X_test.txt'\n",
    "    y_test = 'y_test.txt'\n",
    "    x_train = 'X_train.txt'\n",
    "    y_train = 'y_train.txt'\n",
    "    letters = []\n",
    "    for letter_a in string.ascii_letters:\n",
    "        for letter_b in string.ascii_letters[:11]:\n",
    "            letters.append(letter_a+letter_b)\n",
    "    letters = letters[:561]\n",
    "    test_id = pd.read_csv(y_test, names=['Index'])\n",
    "    test_df = pd.read_csv(x_test, delim_whitespace=True, names=letters)\n",
    "    test_df = test_df.set_index(keys=test_id['Index'], drop=True)\n",
    "    train_id = pd.read_csv(y_train, names=['Index'])\n",
    "    train_df = pd.read_csv(x_train, delim_whitespace=True, names=letters)\n",
    "    train_df = train_df.set_index(keys=train_id['Index'], drop=True)\n",
    "    activity_df = test_df.append(train_df)\n",
    "    activity_df = activity_df.loc[:, ['aa','ab','ac','Yh', 'Yi', 'Yj']]\n",
    "    print(activity_df)\n",
    "    data_info = estimate_cov_mu(data=activity_df)\n",
    "    print(data_info)\n",
    "    plot_subset(data=activity_df, subset=['aa', 'ab', 'ac'])\n",
    "    plot_subset(data=activity_df, subset=['Yh', 'Yi', 'Yj'])\n",
    "    activity_df = make_decisions(data=activity_df, data_info=data_info)\n",
    "    print(activity_df)\n",
    "    write_sample_data(activity_df, './activity_data.csv')\n",
    "    plot_correct_classified(data=activity_df, subset=['aa', 'ab', 'ac'])\n",
    "    plot_correct_classified(data=activity_df, subset=['Yh', 'Yi', 'Yj'])\n",
    "    plot_decision_matrix(data=activity_df, save_path='./activity_decision_matrix.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3239990",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_har_test = pd.read_fwf(x_test,delimiter=' ')\n",
    "x_har_train = pd.read_fwf(x_train,delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cba7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_har_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6676aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_har_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787af778",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_exprmnt = 'winequality-white.csv'\n",
    "wine_df_exprmnt = pd.read_csv(wine_exprmnt, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df_exprmnt_y = wine_df_exprmnt['quality']\n",
    "wine_df_exprmnt = wine_df_exprmnt.drop('quality',axis = 1)\n",
    "wine_df_exprmnt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_df_exprmnt\n",
    "Y = wine_df_exprmnt_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0da02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ccb8ea",
   "metadata": {},
   "source": [
    "**PCA-Wine Quality for Train Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1])\n",
    "plt.xlabel(\"z1\")\n",
    "plt.ylabel(\"z2\")\n",
    "plt.title(\"PCA projections to 2D space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4336f",
   "metadata": {},
   "source": [
    "**PCA-Wine Quality for Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1])\n",
    "plt.xlabel(\"z1\")\n",
    "plt.ylabel(\"z2\")\n",
    "plt.title(\"PCA projections to 2D space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First derive sample-based estimates of mean vector and covariance matrix:\n",
    "mu_hat = np.mean(X, axis=0)\n",
    "Sigma_hat = np.cov(X.T)\n",
    "\n",
    "# Mean-subtraction is a necessary assumption for PCA, so perform this to obtain zero-mean sample set\n",
    "C = X - mu_hat\n",
    "\n",
    "# Get the eigenvectors (in U) and eigenvalues (in D) of the estimated covariance matrix\n",
    "lambdas, U = np.linalg.eig(Sigma_hat)\n",
    "\n",
    "# Get the indices from sorting lambdas in order of increasing value, with ::-1 slicing to then reverse order\n",
    "idx = lambdas.argsort()[::-1]\n",
    "\n",
    "# Extract corresponding sorted eigenvectors and eigenvalues\n",
    "U = U[:, idx]\n",
    "D = np.diag(lambdas[idx])\n",
    "\n",
    "# Calculate the PC projections of zero-mean samples (in z)\n",
    "Z = C.dot(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390bcbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what it looks like only along the first two PCs\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(Z[0], Z[1])\n",
    "plt.xlabel(\"z1\")\n",
    "plt.ylabel(\"z2\")\n",
    "plt.title(\"PCA projections to 2D space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffad814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_har_test = sc.fit_transform(x_har_test)\n",
    "x_har_train = sc.transform(x_har_train)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "x_har_test = pca.fit_transform(x_har_test)\n",
    "x_har_train = pca.transform(x_har_train)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175fbea",
   "metadata": {},
   "source": [
    "**PCA- HAR Dataset Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(x_har_test[:, 0], x_har_test[:, 1])\n",
    "plt.xlabel(\"z1\")\n",
    "plt.ylabel(\"z2\")\n",
    "plt.title(\"PCA projections to 2D space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c5b4d",
   "metadata": {},
   "source": [
    "**PCA- HAR Dataset Train Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb04d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(x_har_train[:, 0], x_har_train[:, 1])\n",
    "plt.xlabel(\"z1\")\n",
    "plt.ylabel(\"z2\")\n",
    "plt.title(\"PCA projections to 2D space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d4187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
