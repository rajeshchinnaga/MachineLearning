{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edcf446",
   "metadata": {},
   "source": [
    "##  <center>Assignment-2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68c977",
   "metadata": {},
   "source": [
    "**Question-1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b70fe",
   "metadata": {},
   "source": [
    "Given:\n",
    "1. class priors probablities: p(L = 0) = 0.65 and p(L = 1) = 0.35\n",
    "2. Mean vectors:\n",
    "    1. $$ \\mu_{01} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}$$ <br> \n",
    "    2. $$ \\mu_{02} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}$$ <br> \n",
    "    3. $$ \\mu_{1} = \\begin{bmatrix}  2 \\\\ 2 \\end{bmatrix}$$ <br> \n",
    "3. Covariance Matrices:\n",
    "    1. $$ \\Sigma_{01} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$$ <br> \n",
    "    2. $$ \\Sigma_{02} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix}$$ <br> \n",
    "    3. $$ \\Sigma_{1} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$$ <br> \n",
    "    \n",
    "4. Parameters of the class conditonal Guassian PDF's:\n",
    "    1. $$ a_{1} = 0.5 $$\n",
    "    2. $$ a_{2} = 0.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f07a7",
   "metadata": {},
   "source": [
    "**PART-1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a46eee",
   "metadata": {},
   "source": [
    "I am implementing this using classification rule:\n",
    "\n",
    "$${p(x |L=1) \\over p(x |(L=0)} > \\gamma$$ \n",
    "\n",
    "Solution:\n",
    "ERM CLassifier: with 2 Gaussian Mixture model class conditionals we can derive the above equation to:\n",
    "\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} = \\frac {\\sum_{i=1}^ {\\mu_{1}} \\alpha_{1i} g(x_{i}\\mu_{1i}\\Sigma_{1i})}{\\sum_{i=1}^ {\\mu_{0}} \\alpha_{0i} g(x_{i}\\mu_{0i}\\Sigma_{0i}))}$$\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} = \\frac {g(x|m1,C1)}{g(x|m0,C0)} $$\n",
    "\n",
    "where g(x|m,C) is a multi variate gaussian probablity density function with mean vector m and covariance matrix C.\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} = \\frac {g(x|m1,C1)}{g(x|m0,C0)}  > \\gamma$$\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} = \\frac {g(x|m1,C1)}{g(x|m0,C0)}  > \\gamma = \\frac {p(x|L=0)}{p(x|L=1)} * \\frac {\\delta_{01}-\\delta_{00}}{\\delta_{10}-\\delta_{11}}$$\n",
    "\n",
    "$$ \\gamma = \\frac {p(L=0)}{p(L=1)} * \\frac {1 - 0}{1 - 0}$$\n",
    "$$ \\implies \\gamma = \\frac {0.65}{0.35} $$\n",
    "$$ \\implies \\gamma = 1.8572 $$\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} > \\gamma $$\n",
    "$$ \\implies {p(x |L=1) \\over p(x |(L=0)} > 1.8572 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06a14d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.optimize as spo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b4bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "priors = [0.65, 0.35]\n",
    "weights = [.5, .5]\n",
    "class0_mean = [[3,0],[0,3]]\n",
    "class0_cov = [[[2,0],[0,1]],[[1,0],[0,2]]]\n",
    "class1_mean = [2,2]\n",
    "class1_cov = [[1,0],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7181f1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateData(numSamples):\n",
    "    # Generate Samples\n",
    "    labels = []\n",
    "    samples = []\n",
    "    for n in range(0, numSamples):\n",
    "        val = random.random()\n",
    "        if val < priors[0]:\n",
    "            labels.append(0)\n",
    "            if val < priors[0] / 2:\n",
    "                samples.append(np.random.multivariate_normal(class0_mean[0], class0_cov[0]))\n",
    "            else:\n",
    "                samples.append(np.random.multivariate_normal(class0_mean[1], class0_cov[1]))\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            samples.append(np.random.multivariate_normal(class1_mean, class1_cov))\n",
    "\n",
    "    return samples,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0993b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generating samples and validation set with labes:\n",
    "D20_train_data, D20_train_labels = generateData(20)\n",
    "D200_train_data, D200_train_labels = generateData(200)\n",
    "D2000_train_data, D2000_train_labels = generateData(2000)\n",
    "D10k_validate_data, D10k_validate_labels = generateData(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cef021",
   "metadata": {},
   "source": [
    "Validation set plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372db55a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plotting validation set:\n",
    "samp0_10k = [x for (i, x) in enumerate(D10k_validate_data) if D10k_validate_labels[i] == 0]\n",
    "samp1_10k = [x for (i, x) in enumerate(D10k_validate_data) if D10k_validate_labels[i] == 1]\n",
    "plt.plot([x[0] for x in samp0_10k], [x[1] for x in samp0_10k], '.', color='blue', label='Class 0')\n",
    "plt.plot([x[0] for x in samp1_10k], [x[1] for x in samp1_10k], '.', color='red', label='Class 1')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Class 0(blue) and Class 1(orange) True Class Labels')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff23b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Question 1 Part 1:\n",
    "discrim = []\n",
    "\n",
    "for s in D10k_validate_data:\n",
    "    disc = multivariate_normal.pdf(s, class1_mean, class1_cov) / (0.5 * multivariate_normal.pdf(s, class0_mean[0], class0_cov[0]) + 0.5 * multivariate_normal.pdf(s, class0_mean[1], class0_cov[1]))\n",
    "    discrim.append(disc)\n",
    "\n",
    "labelDiscrims10k = pd.DataFrame([D10k_validate_labels, discrim])\n",
    "#Taking Transpose\n",
    "labelDiscrims10k = labelDiscrims10k.transpose()\n",
    "labelDiscrims10k.columns = ['labels', 'discrims']\n",
    "# Create Gamma thresholds for ROC curve\n",
    "sortedDisc = sorted(discrim)\n",
    "gammaVals = [0]\n",
    "for i, d in enumerate(sortedDisc[0:-1]):\n",
    "    gammaVals.append((sortedDisc[i]+sortedDisc[i+1])/2.0)\n",
    "\n",
    "gammaVals.append(sortedDisc[-1] + 1)    # Add a gamma threshold greater than all descriminants\n",
    "gammas = sorted(gammaVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06c173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating False and true positive rates\n",
    "class0Count = len(samp0_10k)\n",
    "class1Count = len(samp1_10k)\n",
    "falsePosRate = []\n",
    "truePosRate = []\n",
    "perError = []\n",
    "\n",
    "numGammas = len(gammas)\n",
    "for ind in range(numGammas):\n",
    "    trueNegCount = labelDiscrims10k[(labelDiscrims10k['labels'] == 0) & (labelDiscrims10k['discrims'] < gammas[ind])].shape[0]\n",
    "    falseNegCount = labelDiscrims10k[(labelDiscrims10k['labels'] == 1) & (labelDiscrims10k['discrims'] < gammas[ind])].shape[0]\n",
    "    falsePosCount = labelDiscrims10k[(labelDiscrims10k['labels'] == 0) & (labelDiscrims10k['discrims'] > gammas[ind])].shape[0]\n",
    "    truePosCount = labelDiscrims10k[(labelDiscrims10k['labels'] == 1) & (labelDiscrims10k['discrims'] > gammas[ind])].shape[0]\n",
    "    falsePosRate.append(falsePosCount/class0Count)\n",
    "    truePosRate.append(truePosCount/class1Count)\n",
    "    perError.append((falsePosCount + falseNegCount)/10000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ed6a2",
   "metadata": {},
   "source": [
    "Implementing the classifier rule, below are the theoretical and practical values obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f43d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Minimum error for gamma\n",
    "minError = min(perError)\n",
    "indMinError = perError.index(minError)\n",
    "print(\"Gamma (minimum error): \" + str(gammas[indMinError]))\n",
    "print(\"Minimum probability of error: \" + str(minError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289edb8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find theoretical false positive rate, true positive rate, and error\n",
    "theoDisc = priors[0]/priors[1]\n",
    "theoTrueNegCount = labelDiscrims10k[(labelDiscrims10k['labels'] == 0) & (labelDiscrims10k['discrims'] < theoDisc)].shape[0]\n",
    "theoFalseNegCount = labelDiscrims10k[(labelDiscrims10k['labels'] == 1) & (labelDiscrims10k['discrims'] < theoDisc)].shape[0]\n",
    "theoFalsePosCount = labelDiscrims10k[(labelDiscrims10k['labels'] == 0) & (labelDiscrims10k['discrims'] > theoDisc)].shape[0]\n",
    "theoTruePosCount = labelDiscrims10k[(labelDiscrims10k['labels'] == 1) & (labelDiscrims10k['discrims'] > theoDisc)].shape[0]\n",
    "theoFalsePosRate = theoFalsePosCount/class0Count\n",
    "theoTruePosRate = theoTruePosCount/class1Count\n",
    "theoError = (theoFalsePosCount + theoFalseNegCount)/10000.0\n",
    "print(\"Theoretical optimal gamma is: \" + str(theoDisc))\n",
    "print(\"Theoretical minimum probability of error is: \" + str(theoError))\n",
    "print(\"Experimental Minimum error: \" +str(truePosRate[indMinError]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e55f1",
   "metadata": {},
   "source": [
    "Plotting ROC Curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c1229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.plot(falsePosRate, truePosRate, label='ROC curve')\n",
    "plt.plot(falsePosRate[indMinError], truePosRate[indMinError], 'bo', label='Experimental Minimum Error')\n",
    "plt.plot(theoFalsePosRate, theoTruePosRate, 'ro', label='Theoretical Minimum Error')\n",
    "plt.ylabel('P(True Positive)')\n",
    "plt.xlabel('P(False Positive)')\n",
    "plt.title('Minimum Expected Risk ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf672d82",
   "metadata": {},
   "source": [
    "**PART-2(A)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc80386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logisticFunctionClassificationLikelihood(modelParam, trainData, trainLablels, fit):\n",
    "    if fit == 'linear':\n",
    "        z = [np.r_[1, samp] for samp in trainData]\n",
    "    elif fit == 'quadratic':\n",
    "        z = [np.r_[1, samp[0], samp[1], samp[0] ** 2, samp[0] * samp[1], samp[1] ** 2] for samp in trainData]\n",
    "    else:\n",
    "        print('Unknown fit type for logistic classification')\n",
    "        exit(-1)\n",
    "        return\n",
    "  \n",
    "    logVals = [1.0/(1 + np.exp(np.matmul(modelParam, z[samp]))) for samp in range(len(trainData))]\n",
    "\n",
    "    correctLiklihood = [(1-logVals[i]) if trainLablels[i] == 0 else logVals[i] for i in range(len(trainData))]\n",
    "\n",
    "    return -1 * np.mean(np.log(correctLiklihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d6035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimizeLogisticClassification(trainData, trainLabels, initParams, fit):\n",
    "    optimizeResult = spo.minimize(fun=logisticFunctionClassificationLikelihood, x0=initParams,\n",
    "                                  args=(trainData, trainLabels, fit), method='Nelder-Mead')\n",
    "\n",
    "    if not optimizeResult.success:\n",
    "        print(optimizeResult.message)\n",
    "        exit(-1)\n",
    "    return optimizeResult.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9cef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotLogClassPerformance(params, fit, info=''):\n",
    "    #Checking which classification to use:\n",
    "    if fit == 'linear':\n",
    "        likelihoods = [params[0] + params[1] * D10k_validate_data[i][0] +\n",
    "                       params[2] * D10k_validate_data[i][1] for i in range(10000)]\n",
    "    elif fit == 'quadratic':\n",
    "        likelihoods = [params[0] + params[1] * D10k_validate_data[i][0] +\n",
    "                       params[2] * D10k_validate_data[i][1] +\n",
    "                       params[3] * (D10k_validate_data[i][0] ** 2) +\n",
    "                       params[4] * D10k_validate_data[i][0] * D10k_validate_data[i][1] +\n",
    "                       params[5] * (D10k_validate_data[i][1] ** 2) for i in range(10000)]\n",
    "    else:\n",
    "        print('Unknown Fit Type')\n",
    "        exit(-1)\n",
    "        return\n",
    "\n",
    "    decisions = [int(i < 0.5) for i in likelihoods]\n",
    "    numErrors = 0\n",
    "    for i in range(10000):\n",
    "        if decisions[i] != D10k_validate_labels[i]:\n",
    "            numErrors += 1\n",
    "    errorProb = numErrors/10000.0\n",
    "    print('Probability of error ' + info +': ' + str(errorProb))\n",
    "\n",
    "    # Plot Data As Classified Correct or Incorrect\n",
    "    class0Correct = [s for (i, s) in enumerate(D10k_validate_data) if D10k_validate_labels[i] == 0 and\n",
    "                     decisions[i] == 0]\n",
    "    class0Incorrect = [s for (i, s) in enumerate(D10k_validate_data) if D10k_validate_labels[i] == 0 and\n",
    "                       decisions[i] == 1]\n",
    "    class1Correct = [s for (i, s) in enumerate(D10k_validate_data) if D10k_validate_labels[i] == 1 and\n",
    "                     decisions[i] == 1]\n",
    "    class1Incorrect = [s for (i, s) in enumerate(D10k_validate_data) if D10k_validate_labels[i] == 1 and\n",
    "                       decisions[i] == 0]\n",
    "\n",
    "    plt.plot([x[0] for x in class0Correct], [x[1] for x in class0Correct], '^', color='blue',\n",
    "             label='Class 0 Correct')\n",
    "    plt.plot([x[0] for x in class0Incorrect], [x[1] for x in class0Incorrect], '^', color='red',\n",
    "             label='Class 0 Incorrect')\n",
    "    plt.plot([x[0] for x in class1Correct], [x[1] for x in class1Correct], 'o', color='blue',\n",
    "             label='Class 1 Correct')\n",
    "    plt.plot([x[0] for x in class1Incorrect], [x[1] for x in class1Incorrect], 'o', color='red',\n",
    "             label='Class 1 Incorrect')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Logistic ' + fit + ' Classification ' + info)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa10ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelParamLin = np.array([0, 0, 0])\n",
    "modelParamLin = optimizeLogisticClassification(D20_train_data, D20_train_labels, modelParamLin, 'linear')\n",
    "print(modelParamLin)\n",
    "plotLogClassPerformance(modelParamLin, 'linear', '20 Train Samples')\n",
    "modelParamLin = optimizeLogisticClassification(D200_train_data, D200_train_labels, modelParamLin, 'linear')\n",
    "print(modelParamLin)\n",
    "plotLogClassPerformance(modelParamLin, 'linear', '200 Train Samples')\n",
    "modelParamLin = optimizeLogisticClassification(D2000_train_data, D2000_train_labels, modelParamLin, 'linear')\n",
    "print(modelParamLin)\n",
    "plotLogClassPerformance(modelParamLin, 'linear', '2k Train Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f425d",
   "metadata": {},
   "source": [
    "**PART-2(B)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5789b0f",
   "metadata": {},
   "source": [
    "Logistic quadratic function based approximations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73306bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using logistic quadratic classification\n",
    "modelParamQuad = np.array([0, 0, 0, 0, 0, 0])\n",
    "modelParamQuad = optimizeLogisticClassification(D20_train_data, D20_train_labels, modelParamQuad, 'quadratic')\n",
    "print(modelParamQuad)\n",
    "plotLogClassPerformance(modelParamQuad, 'quadratic', '20 Train Samples')\n",
    "modelParamQuad = optimizeLogisticClassification(D200_train_data, D200_train_labels, modelParamQuad, 'quadratic')\n",
    "print(modelParamQuad)\n",
    "plotLogClassPerformance(modelParamQuad, 'quadratic', '200 Train Samples')\n",
    "modelParamQuad = optimizeLogisticClassification(D2000_train_data, D2000_train_labels, modelParamQuad, 'quadratic')\n",
    "print(modelParamQuad)\n",
    "plotLogClassPerformance(modelParamQuad, 'quadratic', '2k Train Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fca0c2",
   "metadata": {},
   "source": [
    "**Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5451d",
   "metadata": {},
   "source": [
    "1. Logistic Linear:\n",
    "    1. Probability of error for 20 train samples    - 0.5471\n",
    "    2. Probability of error for 200 train samples   - 0.3675\n",
    "    3. Probability of error for 2000 train samples  - 0.3619\n",
    "2. Logistic quadratic:\n",
    "    1. Probability of error for 20 train samples    - 0.1785 \n",
    "    2. Probability of error for 200 train samples   - 0.1855\n",
    "    3. Probability of error for 2000 train samples  - 0.1840\n",
    "    \n",
    "Based on the above probability of errors we can conclude that logistic linear model is a bad fit for the given data and logistic quadratic is a good fit.\n",
    "\n",
    "Comparing with part-1:\n",
    "1. Theoretical optimal gamma is: 1.8571428571428574\n",
    "2. Theoretical minimum probability of error is: 0.1733\n",
    "3. Experimental Minimum error: 0.7759593679458239\n",
    "\n",
    "Logistic quadratic is having slight deviation when compared to probability of minimum error with theoretical minimum probability of error and has much difference than logistic linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018e4cd",
   "metadata": {},
   "source": [
    "**Question-2** <br>\n",
    "<br>\n",
    "Maximum Likelihood(ML) and Maximum-A-Posteriori(MAP) Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b18d6",
   "metadata": {},
   "source": [
    "Given: <br>\n",
    "A scalar-real y and two-dimensional real vector x are related to each other y = c(x, w ) + v where c(., w ) is a cubic polynomial in x with coefficients w , and $\\epsilon$ is a random Gaussian scalar with mean zero and $\\sigma^{2}$ variance $(\\epsilon\\sim N(0,\\sigma^{2}))$\n",
    "\n",
    "Let<br>\n",
    "$w=\\left[ w_{0},w_{1},w_{2},w_{3},w_{4},w_{5},w_{6} \\right]$<br>\n",
    "$\\phi(x_{n})=\\left[ 1,x_{1},x_{2},x_{1}^{2},x_{2}^{2},x_{1}^{3},x_{2}^{3} \\right]$<br>\n",
    "Let Dataset $D=(x_{1},y_{1}),(x_{2},y_{2})...(x_{N},y_{N})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ffaa7",
   "metadata": {},
   "source": [
    "Implementing Maximum Likelihood Estimate(MLE) for Gaussian distribution:\n",
    "    $argmax_{w}\\prod_{n=1}^{N}p(t|\\phi(x_{n}),w,\\beta)=argmax_{w}\\prod_{n=1}^{N}\\frac{1}{\\sigma\\sqrt{2\\pi}}exp (-\\frac{1}{2}(\\frac{t_{n}-w^{T}\\phi(x_{n})}{\\sigma})^{2})$\n",
    "where \n",
    "$\\beta=\\frac{1}{\\sigma^{2}}$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Taking natural log of the function:\n",
    "    $argmax_{w}\\prod_{n=1}^{N}p(t|\\phi(x_{n}),w,\\beta)=\\frac{N}{2}ln\\beta-\\frac{N}{2}ln2\\pi-\\beta E_{D}(w)$\n",
    "\n",
    "where \n",
    "$\\beta=\\frac{1}{\\sigma^{2}}$\n",
    "<br><br>\n",
    "$E_{d}(w)=\\frac{1}{2}\\sum_{n=1}^{N}(t_{n}-w^{T}\\phi (x_{n}))^{2}$ is squared error\n",
    "<br><br>\n",
    "Set the gradient to zero and solving for the $w$ vector to calculate the maximum probability:\n",
    "<br>\n",
    "$\\hat{w}_{ML}=0=(\\phi^{T}\\phi)^{-1}\\phi^{T}\\hat{t}$<br><br>\n",
    "Maximum Likelihood Estimate (MLE):\n",
    "$\\implies{w}_{ML}=(\\phi^{T}\\phi)^{-1}\\phi^{T}\\hat{t}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc0ba2",
   "metadata": {},
   "source": [
    "Deriving MAP estimate for a Gaussian Distribution: <br>\n",
    "\n",
    "Gaussian prior distribution over the weight vector w is defined with the below forumla:\n",
    "\n",
    "$$ P(w) = Nor(w | 0, \\lambda^{-1}I) = \\frac{1}{(2\\pi)^{D/2}}exp(-\\frac{\\lambda}{2}w^Tw) $$ <br>\n",
    "\n",
    "Log posterior probability: <br>\n",
    "$$ log(P(w | D)) = log \\frac{P(w)P(D|w)}{P(D)} $$ <br>\n",
    "$$\\implies log(P(w | D)) = log(P(w)) + log(P(D | w)) - log (P(D)) $$<br>\n",
    "\n",
    "Maximum-A-Posteriori Estimate:\n",
    "\n",
    "$$\\hat{w}_{MAP} = arg max_{w} log(P(w | D)) $$<br>\n",
    "$$\\implies \\hat{w}_{MAP} = arg max_{w} log(P(w))+log(P(D | w))-logP(D) $$<br>\n",
    "$$\\implies \\hat{w}_{MAP} = arg max_{w} log(P(w))+log(P(D | w)) $$<br>\n",
    "$$\\implies \\hat{w}_{MAP} = arg max_{w}(-\\frac{D}{2} log(2\\pi) - \\frac{\\lambda}{2}w^T w + \\Sigma_{n=1}^{N} (-\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{(t_{n}-w^T \\phi(x_{n}))^2}{2\\sigma^2} )) $$ <br>\n",
    "where $$\\Sigma_{n=1}^{N} (-\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{(t_{n}-w^T \\phi(x_{n}))^2}{2\\sigma^2}$$ is log-Likelihood\n",
    "\n",
    "Set gradient to zero, we get the below expression:<br>\n",
    "\n",
    "$$\\hat{w}_{MAP}= argmax_{w}-\\frac{\\lambda}{2}w_{T}w-\\beta E_{D}(w) $$\n",
    "where \n",
    "$$\\beta=\\frac{1}{\\sigma^{2}}$$\n",
    "$$E_{d}(w)=\\frac{1}{2}\\sum_{n=1}^{N}(t_{n}-w^{T}\\phi (x_{n}))^{2}$$ is squared error\n",
    "<br><br>\n",
    "$$\\implies \\hat{w}_{MAP} = (\\lambda I+\\phi^{T}\\phi)^{-1}\\phi^{T} \\hat{t}$$\n",
    "Maximum-A-Posteriori Estimate (MAP): $$\\implies \\hat{w}_{MAP} = (\\lambda I+\\phi^{T}\\phi)^{-1}\\phi^{T} \\hat{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dccfd73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "def hw2_q2(N, dataset_name):\n",
    "    gmm_pdf = {}\n",
    "    gmm_pdf['priors'] = np.array([.3, .4, .3])\n",
    "    gmm_pdf['mu'] = np.array([[-10, 0, 10], [0, 0, 0], [10, 0, -10]])  # Gaussian distributions means\n",
    "    gmm_pdf['Sigma'] = np.array([[[1, 0, -3], [0, 1, 0], [-3, 0, 15]], [[8, 0, 0], \n",
    "                                [0, .5, 0], [0, 0, .5]],\n",
    "                                 [[1, 0, -3], [0, 1, 0], [-3, 0, 15]]])  # Gaussian distributions covariance matrices\n",
    "    X, y = generate_gmm_data(N, gmm_pdf)\n",
    "    # Plot the original data and their true labels\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax_raw = fig.add_subplot(111, projection='3d')\n",
    "    ax_raw.scatter(X[:, 0], X[:, 1], y, marker='o', color='b')\n",
    "    ax_raw.set_xlabel(r\"$x_1$\")\n",
    "    ax_raw.set_ylabel(r\"$x_2$\")\n",
    "    ax_raw.set_zlabel(r\"$y$\")\n",
    "    # Set equal axes for 3D plots\n",
    "    ax_raw.set_box_aspect((np.ptp(X[:, 0]), np.ptp(X[:, 1]), np.ptp(y)))\n",
    "    plt.title(\"{} Dataset\".format(dataset_name))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def generate_gmm_data(N, gmm_pdf):\n",
    "    # Generates N vector samples from the specified mixture of Gaussians\n",
    "    # Returns samples and their component labels\n",
    "    # Data dimensionality is determined by the size of mu/Sigma parameters\n",
    "\n",
    "    # Decide randomly which samples will come from each component\n",
    "    u = np.random.random(N)\n",
    "    thresholds = np.cumsum(gmm_pdf['priors'])\n",
    "    thresholds = np.insert(thresholds, 0, 0)  # For intervals of classes\n",
    "\n",
    "    n = gmm_pdf['mu'].shape[0]  # Data dimensionality\n",
    "\n",
    "    X = np.zeros((N, n))\n",
    "    C = len(gmm_pdf['priors'])  # Number of components\n",
    "    for i in range(C + 1):\n",
    "        # Get randomly sampled indices for this Gaussian, checking between thresholds based on class priors\n",
    "        indices = np.argwhere((thresholds[i - 1] <= u) & (u <= thresholds[i]))[:, 0]\n",
    "        # No. of samples in this Gaussian\n",
    "        X[indices, :] = mvn.rvs(gmm_pdf['mu'][i - 1], gmm_pdf['Sigma'][i - 1], len(indices))\n",
    "\n",
    "    return X[:, 0:2], X[:, 2]\n",
    "        \n",
    "def mle(phi, t):\n",
    "    # get pseudo-inverse\n",
    "    tphi = np.transpose(phi)\n",
    "    results = np.matmul(np.linalg.inv(np.matmul(tphi,phi)),tphi)\n",
    "    # multiply by y\n",
    "    results = np.matmul(results, t)\n",
    "    return results\n",
    "\n",
    "def mean_squared_error(w, x_test, y_test):\n",
    "    N = len(y_test)\n",
    "    x = []\n",
    "    for i in range(0,N,1):\n",
    "        row = [1, x_test[0][i], x_test[1][i], x_test[0][i]**2, \n",
    "                x_test[1][i]**2, x_test[0][i]**3, x_test[1][i]**3]\n",
    "        x.append(row)\n",
    "    total_error = 0\n",
    "    for n in range(0,N,1):\n",
    "        error = (y_test[n]-np.dot((w),x[n]))**2\n",
    "        total_error = total_error + error\n",
    "    total_error = total_error/N\n",
    "    return total_error[0,0]\n",
    "\n",
    "def map(phi, gamma, t):\n",
    "    # get pseudo-inverse\n",
    "    tphi = np.transpose(phi)\n",
    "    results = np.matmul(np.linalg.inv(np.add(gamma*np.identity(phi.shape[1]), np.matmul(tphi,phi))),tphi)\n",
    "    # multiply by y\n",
    "    results = np.matmul(results, t)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b414c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    N_train = 100\n",
    "    N_valid = 1000\n",
    "    X_train, y_train = hw2_q2(N_train, \"Training\")\n",
    "    X_valid, y_valid = hw2_q2(N_valid, \"Validation\")\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    phi = []\n",
    "    xT = X_train.transpose()\n",
    "    yT = y_train.transpose()\n",
    "    xV = X_valid.transpose()\n",
    "    yV = y_valid.transpose()\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(2,2, figsize=(10, 10), sharex=False, sharey=True)\n",
    "    for i in range(0,100,1):\n",
    "        row = [1, xT[0][i], xT[1][i], xT[0][i]**2, \n",
    "                xT[1][i]**2, xT[0][i]**3, xT[1][i]**3]\n",
    "        phi.append(row)\n",
    "    phi = np.matrix(phi)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Plot training data\n",
    "    for i in range(0,1000,1):\n",
    "        axes[0,0].plot(xV[0][i], yV[i], color='skyblue', marker='.', alpha=0.4)\n",
    "        axes[0,1].plot(xV[1][i], yV[i], color='palegreen', marker='.', alpha=0.4)\n",
    "        axes[1,0].plot(xV[0][i], yV[i], color='lightcoral', marker = '.', alpha=0.4)\n",
    "        axes[1,1].plot(xV[1][i], yV[i], color='orange', marker='.', alpha=0.4)\n",
    "    min_x_0 = min(xV[0])\n",
    "    max_x_0 = max(xV[0])\n",
    "    min_x_1 = min(xV[1])\n",
    "    max_x_1 = max(xV[1])\n",
    "\n",
    "    # MLE\n",
    "    mle_w = mle(phi, yT)\n",
    "    error = mean_squared_error(mle_w, xV, yV)\n",
    "    print(\"Maximum Likelihood Estimator(MLE):\" ,error)\n",
    "    # Add lines to plots\n",
    "    x_0 = np.linspace(min_x_0, max_x_0, num=1000)\n",
    "    x_1 = np.linspace(min_x_1, max_x_1, num=1000)\n",
    "    w = np.transpose(mle_w)[:,0]\n",
    "    fx = []\n",
    "    for i in range(len(x_0)):\n",
    "        x = x_0[i]\n",
    "        # w: 0,1,3,5\n",
    "        fx.append(w[0]+w[1]*x+w[3]*x**2+w[5]*x**3)\n",
    "    fx = np.squeeze(fx)\n",
    "    axes[0,0].plot(x_0,fx)\n",
    "    fx = []\n",
    "    for i in range(len(x_1)):\n",
    "        x = x_1[i]\n",
    "        # w: 0,1,3,5\n",
    "        fx.append(w[0]+w[2]*x+w[4]*x**2+w[6]*x**3)\n",
    "    fx = np.squeeze(fx)\n",
    "    axes[0,1].plot(x_1,fx)\n",
    "\n",
    "\n",
    "    # MAP\n",
    "    gammas = [i for i in np.logspace(-4,4,9)]\n",
    "    for gamma in gammas:\n",
    "        print(\"Gamma Value\", gamma)\n",
    "        map_w = map(phi, gamma, yT)\n",
    "        error = mean_squared_error(map_w, xV, yV)\n",
    "        print(\"Maximum-A-Posteriori(MAP) estimator\", error)\n",
    "        w = np.transpose(map_w)[:,0]\n",
    "        fx = []\n",
    "        for i in range(len(x_0)):\n",
    "            x = x_0[i]\n",
    "            # w: 0,1,3,5\n",
    "            fx.append(w[0]+w[1]*x+w[3]*x**2+w[5]*x**3)\n",
    "        fx = np.squeeze(fx)\n",
    "        axes[1,0].plot(x_0,fx)\n",
    "        fx = []\n",
    "        for i in range(len(x_1)):\n",
    "            x = x_1[i]\n",
    "            # w: 0,1,3,5\n",
    "            fx.append(w[0]+w[2]*x+w[4]*x**2+w[6]*x**3)\n",
    "        fx = np.squeeze(fx)\n",
    "        axes[1,1].plot(x_1,fx)\n",
    "    axes[0,0].set_ylabel('MLE')\n",
    "    axes[1,0].set_ylabel('MAP')\n",
    "    axes[1,0].set_xlabel('Validate, (x_1,y)')\n",
    "    axes[1,1].set_xlabel('Validate, (x_2,y)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec67192",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Generated Datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be5fa0",
   "metadata": {},
   "source": [
    "Outputs of the MLE trained model and Mean squared errors for varied $\\gamma$ values in the case of MAP trained model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d73ac",
   "metadata": {},
   "source": [
    "Outputs of the Validation data scattered:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aafb59",
   "metadata": {},
   "source": [
    "Obtained and assessed the estimators using the train and validate data sets produced by the given python scripts.\n",
    "1. The Mean Squared Error for the MLE trained model is: 4.625323848260489\n",
    "2. Both the worst performing MLE trained model and the worst performing MAP trained model were equally awful. This is true since the MLE estimate requires a uniform prior which is comparable to gamma of 0.\n",
    "3. As gamma approaches 0, the performance of the MAP estimate resembles that of the MLE estimate.\n",
    "4. Model trained using MAP performed better when the gamma was higher this is due to an increase in the regularization to the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b5c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
